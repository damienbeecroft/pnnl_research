
% ======================================================================
% starting package maintenance...
% installation directory: C:\Users\damie\AppData\Local\Programs\MiKTeX
% package repository: https://ctan.math.illinois.edu/systems/win32/miktex/tm/packages/
% package repository digest: d8b7508a09a0813d998f77c3ce88406c
% going to download 74297 bytes
% going to install 42 file(s) (1 package(s))
% downloading https://ctan.math.illinois.edu/systems/win32/miktex/tm/packages/latexindent.tar.lzma...
% 0.07 MB, 1.03 Mbit/s
% extracting files from latexindent.tar.lzma...
% ======================================================================
\documentclass[12pt]{article}

% PACKAGES
\usepackage[margin = 0.6in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
% MACROS
% Set Theory
\def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\Z{\mathbb{Z}}
%\def\^{\hat}
\def\-{\vec}
\def\d{\partial}
\def\!{\boldsymbol}
\def\X{\times}
%\def\-{\bar}
\def\bf{\textbf}
\def\l{\left}
\def\r{\right}
\def\~{\tilde}
\date{August 2022}
\doublespacing
\title{Project Proposals}
\author{Damien Beecroft}
\date{August 2022}
\begin{document}
\maketitle
\section*{Introduction}
How does one appropriately combine the vast wealth of theory from numerical analysis and dynamical systems with machine learning and data science to most efficiently and accurately solve differential equations? This is a complex problem whose answer depends heavily upon the equation, the available data, and a multitude of other factors. I want to focus my thesis work around answering this question. There are many new machine algorithms coming out for solving machine 


\newpage
\section*{Project Ideas}
\subsection*{Project One}
Kochkov et al. \cite{fluidml} examines the usage of machine learning to improve finite volume method approximations of fluids 
with the availability of high accuracy solution data. They report that their method is roughly eighty times faster than the classical finite volume method.
Furthermore, their algorithm is applied locally and can therefore be scaled to be used for different initial and boundary conditions.
Can similar results be achieved when one has little or no solution data to train on?
Can the benefits of machine learning be made robust or does one need to cherry pick results in order to see improvement? In this first project I will extend upon the research of Kochkov et al. \cite{fluidml} to ascertain the interplay between PINNs and numerical analysis. The first step is to select a series of test problems.


The first and central project is an analysis of a multifidelity finite basis PINN (MFFBPINN) with a numerical method as the single-fidelity solver. For this project I would choose one differential equation
to analyze--most likely the two-dimensional Navier-Stokes equation at a reasonable Reynold's number--and perform a wide array of tests on
this sample problem. 
\begin{align*}
    u_t + u u_x + v u_y - \nu u_{xx} &= - \frac{p_x}{\rho} \\
    v_t + u v_x + v v_y - \nu v_{yy} &= - \frac{p_y}{\rho} 
\end{align*}
Navier-Stokes is an ideal test problem because classical numerical
methods are known to struggle simulating fluids. Furthermore, the dynamics depend on the local state of the differential
equation. This means our our neural network--if trained properly--will have an easier time generalizing to problems outside
of the training domain. I intend to use a finite volume method as the numerical solver.
There are two main numerical experiments I want to perform in this paper. The first experiment will be to vary the 
ratio of computational resources that are allocated to the numerical solver versus the PINNs while keeping the total 
computation time and available solution data constant. The second experiment will be to fix the structure of the MFFBPINN while changing the amount of
solution data that is available. I will use the results from these experiments to address the following questions.
What are the trade-offs when one is choosing between funneling computational resources into neural networks as opposed to 
a finer mesh? When do neural networks become a useful tool for modelling? Can MFFBPINNs correct for the biases 
in numerical solvers e.g. the artificial viscosity? Can I reproduce the results of Kochkov et al. \cite{fluidml} in the scenario where only
training data is used? It would also be interesting to repeat these experiments on more 
complex domains where we cannot leverage grid regularity e.g. flow around a complicated shape or through an irregular pipe. 
How does this change the efficacy of machine learning?

The last problem I want to address is a theoretical one. Assuming that the numerical solver is sufficiently accurate, the MFFBPINN be close to the 
true solution even before initialization. Therefore, analyzing the linearized loss landscape should give a good approximation of where the true solution 
is as well as how well conditioned the problem is. Furthermore, when one trains a PINN they sample the residual at a finite number of collocation 
points from an uncountably infinite number of domain points. How large does the batch need to be so that we can expect that the neural network will 
improve upon the numerical solver?


% In this project I will the vary amount of available solution data for training and the ratio of 
% computational resources that are allocated to the numerical solver versus the PINNs.




% to reveal the effects on the accuracy and efficiency of the solver.
% The first and central project is to use a numerical method as the single fidelity solver for a 
% multifidelity finite basis PINN (MFFBPINN). My first test problem will be on an equation whose dynamics are
% determined by the local state. Potential candidates are Navier-Stokes


% I will start by choosing a set of test problems and 
% running numerical methods to ascertain their speed and accuracy. Then, I will take these numerical 
% methods and make them the single fidelity solvers for MFFBPINNs. I will then do a comparison 
% between the classical numerical methods and the MFFBPINNs. Can machine learning push the numerical
% method past the round off error that occurs due to the inherent ill conditioning of numerical methods?
% How do the timings of the methods compare? Would it be simply more beneficial to make a finer grid as opposed to 
% adding a neural network? Are the results different if the problem domain is not regular and grid collocation 
% is no longer an easy task? What if we have access to solution data or theoretical knowledge of fixed point? Can MFFBPINNs correct for the biases 
% in numerical solvers e.g. the artificial viscosity introduced in some finite volume
% methods for the inviscid Burger's equation?
\subsection*{Project Two}
After the PINN experiments are done I want to repeat the studies from project one with multifidelity finite basis DeepONets. 
DeepONets do not work well alone. I wonder whether the aide of numerical analysis can make DeepONets viable. This is the 
very important experiment. Having to retrain a network for every set of initial conditions is unreasonable in a wide range of applications.
\subsection*{Project Three}
I think it would be interesting to look at the loss landscape of the MFFBPINNs with numerical solvers. We know that at 
the beginning of training you are within a certain error $\epsilon$ of the true solution. This means that evaluating a linear
or quadratic approximation of the neural network at initialization would likely give one a good approximation of the loss landscape at the minima. Furthermore, there is something that has struck me as a bit strange about the PINN training process.
In numerical methods the density of grid points is paramount to successful convergence. However, density of collocation points in batches for PINNs is not--at least in my experience--treated with the same importance. When one creates a batch of collocation points 
I believe they should be sampled at or above the Nyquist frequency of the highest frequency mode in the solution. I am curious to see 
whether my intuition can be tested by analyzing the loss landscape. It may also be interesting to analyze the convergence of MFFBPINNs with
numerical solvers through a combination of numerical solvers and the neural tangent kernel.
\bibliographystyle{plain}
\bibliography{refs}
\end{document}


%During the course of this internship, we developed two main ways to implement this algorithm. Each one is centered around how batching is done. In the first implementation, a batch containing points in the entire domain $\Omega$ is sent to the MFFBPINN. In this construction we use a tree of neural networks. The zeroth level creates its solution prediction. The networks on the first level (which are all children of the single fidelity network on the zeroth level) then look at the points in the original batch and the low fidelity solution from the zeroth layer and make their own approximation for the points that are within their subdomains. The child of each network in the second level then take in the batch points and approximations of their parents and create new approximations on the points in their subdomains and so on and so forth. A diagram illustrating this process is given in figure \ref{fig:batch1}. This method scales well with higher dimension and is rather flexible with respect to the shape of the subdomains.

%In the second implementation of the MFFBPINN, distinct batches of residual points are created for each intersection of the subdomains on the highest level. These batch points are then fed down to the neural networks on the lower levels to get the low fidelity approximations. This method is illustrated in Figure
% None of the work on physics informed neural networks (PINNs) that I have seen leverages the vast wealth of 
% existing numerical methods.