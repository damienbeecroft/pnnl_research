@misc{fixedpts,
      title={On the Role of Fixed Points of Dynamical Systems in Training Physics-Informed Neural Networks}, 
      author={Franz M. Rohrhofer and Stefan Posch and Clemens Gößnitzer and Bernhard C. Geiger},
      year={2023},
      eprint={2203.13648},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{
fluidml,
author = {Dmitrii Kochkov  and Jamie A. Smith  and Ayya Alieva  and Qing Wang  and Michael P. Brenner  and Stephan Hoyer },
title = {Machine learning–accelerated computational fluid dynamics},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {21},
pages = {e2101784118},
year = {2021},
doi = {10.1073/pnas.2101784118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2101784118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2101784118},
abstract = {Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier–Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10× finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization.}}
