\documentclass{article}
% Choose a conveniently small page size
% PACKAGES
\usepackage[margin = 1in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{hyperref}

% MACROS
% Set Theory
\def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\Z{\mathbb{Z}}
%\def\^{\hat}
\def\-{\vec}
\def\d{\partial}
\def\!{\boldsymbol}
\def\X{\times}
%\def\-{\bar}
\def\bf{\textbf}
\def\l{\left}
\def\r{\right}
\title{Summer Internship for Damien Beecroft}
\author{Advisors: Amanda Howard, Panos Stinis}
\begin{document}
\maketitle
Physics-informed neural networks, or PINNs, have shown great promise in learning the solution to partial differential equations. However, there are still cases where PINNs fail to train. This summer, we will develop new techniques for cases where PINNs fail to train.
\par This document serves only as a suggested outline, and creativity is encouraged. The remaining weeks will be filled in as we see how the results develop. Many extensions are possible, including training deep operator networks (DeepONets) for a range of input parameters, or switching to model other dynamical systems. 
\par The suggested reading for each week consists of background information that you may find helpful as you work. I suggest you write a short, 1-2 sentence, summary of each paper and add it a working document after reading each paper. This will be the introduction to your final report, and later the introduction for a publication based on your work. Articles are available through PNNL when youâ€™re logged into the VPN.
\begin{figure}[H]
\center
\includegraphics[width=0.8\textwidth]{imgs/domain_decomp}
\end{figure}
We will consider a multifidelity domain decomposition approach, where a PINN is trained in the full domain first. Then, the full domain training is used as a low fidelity prediction for a prediction in a subdivided domain. Two PINNs are trained in level 1, and the domains overlap so initial conditions are not needed to train the subdomain that occurs for later time. This process is repeated until the desired accuracy is reached across the domain.  
\par We will be working with Alexander Heinlein, who is an expert in domain decomposition. We will start by testing this setup on the case of an undamped pendulum, to see if we can overcome the issue of converging to fixed points for long time. If successful, we will move to more complex examples. Creativity is encouraged in designing training algorithms. 
\newpage
\subsection*{To Do}
\bf{Reading}:
\begin{itemize}
	\item Parallel Physics-Informed Neural Networks via Domain Decomposition: \url{https://arxiv.org/pdf/2104.10013.pdf}
	\item Fourier Analysis Sheds Light on Deep Neural Networks: \url{https://arxiv.org/abs/1901.06523}
\end{itemize}
\bf{Coding}:
\begin{itemize}
\item Figure out how to run code on Marianas
	\item Pendulum
	\begin{itemize}
		\item Run the domain decomposition code in \verb|Pendulum_DD|
		\item Point selection and evaluation
		\begin{itemize}
			\item Make sure that the code does not evaluate every network at all points. Only evaluate networks on points within the support of their weight function. There are two potential methods for how to do this.
			\begin{itemize}
				\item Change the data generator function to only generate output points in a given domain 
				\item Once you are given a point, check whether or not the point is in the support of the weight function.
			\end{itemize}
			\item Work on parallelization of neural networks defined on different sub-domains.
			\item Testing
			\begin{itemize}
				\item Running with and without the residual driven point selection
				\item Look at the sensitivity of the weights of the loss function
			\end{itemize}
			\item Do testing on how high we can get the maximum time of the pendulum. Currently it is at 10. Can we get it to 20? 50?
		\end{itemize}
	\end{itemize}
	\item Wave Equation
%	\begin{itemize}
%		\item 
%	\end{itemize}
	\item Allen-Cahn
%	\begin{itemize}
%		\item 
%	\end{itemize}
	\item Lorenz
%	\begin{itemize}
%		\item 
%	\end{itemize}
\end{itemize}
\newpage
\newpage
\subsection*{Week 1}
\bf{Videos}:
\begin{itemize}
	\item Stanford CS229M Lecture 13 (Neural Tangent Kernel): \url{https://www.youtube.com/watch?v=btphvvnad0A&list=RDCMUCBa5G_ESCn8Yd4vw5U-gIcg&index=1}
\end{itemize}
\bf{Reading}:
\begin{itemize}
	\item PINNs: \url{https://www.sciencedirect.com/science/article/pii/S0021999118307125}
	\item Multifidelity PINN: \url{https://arxiv.org/pdf/1903.00104.pdf}
	\item Multifidelity DeepONets: \url{https://arxiv.org/pdf/2204.09157.pdf}
	\item Multifidelity Continual Learning: \url{https://arxiv.org/pdf/2304.03894.pdf}
	\item Fixed points and PINNs: \url{https://arxiv.org/pdf/2203.13648.pdf}
	\item Multilevel Domain Decomposition for PINNs: \url{https://arxiv.org/pdf/2306.05486.pdf}
	\item Point selection for PINNs: \url{https://www.sciencedirect.com/science/article/pii/S0045782522006260}
	\item Respecting Causality is All you Need: \url{https://arxiv.org/pdf/2203.07404.pdf}
\end{itemize}
\bf{Coding}:\\
\par I was sent the pendulum code by Amanda. I took some time this week to go through it and get a feel for what is going on. I have not run the code since I do not have my PNNL laptop. I plan to have the code up and running early next week.
\newpage
\subsection*{Week 2}
\bf{Reading}:
\begin{itemize}
	\item A Method for Representing Periodic Functions and Enforcing Exactly Periodic Boundary Conditions with Deep Neural Networks: \url{https://arxiv.org/pdf/2007.07442.pdf}
	\item How and Why PINNs Fail to Train: \url{https://arxiv.org/pdf/2007.14527.pdf}
	\begin{itemize}
		\item Neural Tangent Kernel Convergence and Generalization: \url{https://arxiv.org/abs/1806.07572}
		\item Deep Neural Networks as Gaussian Processes: \url{https://arxiv.org/abs/1711.00165}
	\end{itemize}
\end{itemize}
\bf{Coding}:
\begin{itemize}
\item Get access to Marianas
\item Set up GitHub repository
\item Set up GitHub repository on Marianas and pull code from \verb|pnnl_research|
\item Run Amanda's code locally
\item Plot solutions from Amanda's code
\end{itemize}
\newpage
\subsection*{Week 3} 
\bf{Notes}:
\begin{itemize}
\item I went through the original code Amanda sent me slowly to understand exactly how it works.
\item I went through the altered code in \verb|Pendulum_DD| to understand exactly how it works.
\item I realized that the problem of only applying neural networks to points within their domains can be solved with a rather simple sorting algorithm.
\end{itemize}
\subsection*{Coding}
\begin{itemize}
\item Implementing domain sorting code.
\end{itemize}
\newpage
\subsection*{Week 4}
Lost in GitHub problem
\newpage
\subsection*{Week 5} 
Lost in GitHub problem
%Note: Some updates were lost in a GitHub problem

%\textbf{Code}
%\begin{itemize}
%\item Went to the Seattle PNNL lab to meet Amanda and Sarah.
%\item
%\end{itemize}
\newpage
\subsection*{Week 6}
\textbf{Code}
\begin{itemize}
\item I have begun reading through the JAX documentation in order understand how to implement the multifidelity network architecture.
\end{itemize}
\newpage
\subsection*{Week 7}
\newpage 
\subsection*{Week 8} 
\newpage
\subsection*{Week 9}
\newpage
\subsection*{Week 10}  
\newpage
\end{document}