\documentclass[12pt]{article}

% PACKAGES
\usepackage[margin = 1in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{hyperref}
 \usepackage{setspace}

% MACROS
% Set Theory
\def\N{\mathbb{N}}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\Z{\mathbb{Z}}
%\def\^{\hat}
\def\-{\vec}
\def\d{\partial}
\def\!{\boldsymbol}
\def\X{\times}
%\def\-{\bar}
\def\bf{\textbf}
\def\l{\left}
\def\r{\right}
\date{August 2022}
\doublespacing
\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
            
        \Huge
        \textbf{Multifidelity Finite Basis Physics Informed Neural Networks}
            
            
        \vfill
            
        
        Name: Damien Beecroft\\
        Hosting Site: Pacific Northwest National Lab\\
        Mentors: Amanda Howard and Panos Stinis\\
        \vspace{2cm}
        \begin{flushleft}
       	Mentor's Signature:\\
       	\end{flushleft}
       	\Large
        
            
    \end{center}
\end{titlepage}

\section*{Abstract}
\begin{singlespace}
Physics informed neural networks (PINNs) struggle to successfully learn solutions to differential equations that exhibit high-frequency oscillations or multi-scale behavior. Multilevel finite basis physics informed neural networks (FBPINNs) tackle this problem by recursively discretizing the solution domain and training coupled neural networks on the subdomains. High frequency structures are made less oscillatory with respect to the length scale of these smaller subdomains. This plays to the spectral bias of PINNs and enables one to learn the global solution faster. In this work we integrate multifidelity methods into multilevel FBPINNs to further improve the learning rate. Multifidelity PINNs are provided with both high and low fidelity data and learn the correlation between the two data sources to approximate the global solution. In the multifidelity multilevel FBPINNs framework solution approximations from previous levels supply the low fidelity data. Linear and nonlinear branch networks are then trained to learn the correlation between the low fidelity data and the true solution.\footnote{My internship was virtual, so I didn't get a chance to get any good photos!}
\end{singlespace}
\vfill
\newpage
\section{Introduction}

Analytical and numerical methods for solving differential equations are very 
Scientists and mathematicians have long hunted for efficient methods for solving differential equations.


\section*{Summary of Papers}
\begin{itemize}
\item PINNs: A PINN penalizes the solution if it does not satisfy the physical constraints of the differential equation. PINNs train on a single initial condition.
\item Multifidelity PINNs: These are PINNs that are capable of learning from a combination of low and high fidelity data. To this end, MPINNs learn correlations between the different data fidelitites and use this correlation to overcome sparse high fidelity data.
\item Multifidelity DeepONets: Multifidelity DeepONets use multifidelity data just like MPINNs. However, DeepONets learn the differential equation for all initial conditions.
\item Multifidelity Continual Learning: This method attempts to increase the domain of accuracy of a neural network. Suppose we have a neural net, $\mathcal{NN}_{i-1}$, that can solve a PDE on $\Omega_{i-1}$. $\mathcal{NN}_{i-1}$ is used as a low fidelity approximation train a neural net $\mathcal{NN}_i$ to solve the PDE on a larger set: $\Omega_i$.
\item Fixed Points in PINNs: PINNs have a tendency to converge to fixed points that satisfy the physical constraints of the PDE, but not the initial or boundary conditions. This problem is less prevalent when one trains the PINN on a shorter time interval.
\item MDD for PINNs: This method iteratively partitions the domain of the PDE into a sequence of overlapping sub-domains and trains PINNs on each sub-domains. The PINN trained on the previous level is used as a low-fidelity approximation for the PINNs on the lower levels.
\item Point Selection for PINNs: This paper introduces a probability distribution for sampling new points to compute the PINN residual and improve training.
This paper introduces a probability distribution using the residual that is used to find the best locations to sample the residual and train the PINN.
\item How and Why PINNs Fail to Train: This paper linearizes the neural network about the initial parametrization, $\theta^0$, and performs analysis on the path of the continuous gradient descent of the neural network using this linearization. A variety of very strong assumptions are needed for this analysis to be useful.
\item Respecting Causality: This paper introduces an adjustment to the PINN loss function that encourages the PINN to solve the differential equation at earlier time steps before the later ones. This helps improve performance.
\end{itemize}
\section*{Hard Bias in PINNs to Satisfy Structure of Dynamical Systems}
Within the PINNs 
Physics informed neural networks struggle to capture When analyzing a dynamical system, there are a variety of features that one uses to characterize the solution behavior. These features include
\begin{itemize}
	\item Fixed points: \begin{itemize}
	\item 
	\end{itemize}
\end{itemize}
\pagebreak
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{../mybib} % Entries are in the mybib.bib file
\end{document}

