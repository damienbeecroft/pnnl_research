\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces My internship at Pacific Northwest National Lab was virtual. My office was anywhere I lugged my computer. On my more adventurous days I ventured all the way down to my living room to use the television as a secondary monitor. On one such day my girlfriend Caitlin Neher was kind enough to take this photo for my final report.\relax }}{1}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:me}{{1}{1}{My internship at Pacific Northwest National Lab was virtual. My office was anywhere I lugged my computer. On my more adventurous days I ventured all the way down to my living room to use the television as a secondary monitor. On one such day my girlfriend Caitlin Neher was kind enough to take this photo for my final report.\relax }{figure.caption.2}{}}
\citation{pinns}
\citation{fixedpts}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{eq:dq}{{1}{2}{Introduction}{equation.1.1}{}}
\newlabel{eq:loss}{{1}{2}{Introduction}{equation.1.2}{}}
\citation{mfpinns}
\citation{mfdeeponets}
\citation{fbpinns}
\citation{fbpinns}
\citation{fbpinns}
\citation{fbpinns}
\citation{fbpinns}
\@writefile{toc}{\contentsline {section}{\numberline {2}Description of Project}{3}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces This is an illustration of a multifidelity PINN. The multifidelity PINN takes in low fidelity approximations ($u_{i-1}$) and collocation points ($x$, $t$) and outputs a new set of approximations ($u_i$) that are hopefully better than the low fidelity ones. $u_i$ is called the high fidelity prediction. This network can then be differentiated to see how well it satisfies the differential equation.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:mfpinn}{{2}{3}{This is an illustration of a multifidelity PINN. The multifidelity PINN takes in low fidelity approximations ($u_{i-1}$) and collocation points ($x$, $t$) and outputs a new set of approximations ($u_i$) that are hopefully better than the low fidelity ones. $u_i$ is called the high fidelity prediction. This network can then be differentiated to see how well it satisfies the differential equation.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This is a graphic illustrating multilevel finite basis PINNs from Dolean et al. \cite  {fbpinns}. Level 1 is trained first. Then, level 2 is trained to correct for the residual of level 1. Levels 1 and 2 are averaged to get the new estimate of the global solution. Level 3 is trained to correct for the errors of the new global solution. Levels 1, 2, and 3 are averaged. This process continues until a desired level of accuracy is achieved.\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:fbpinn}{{3}{4}{This is a graphic illustrating multilevel finite basis PINNs from Dolean et al. \cite {fbpinns}. Level 1 is trained first. Then, level 2 is trained to correct for the residual of level 1. Levels 1 and 2 are averaged to get the new estimate of the global solution. Level 3 is trained to correct for the errors of the new global solution. Levels 1, 2, and 3 are averaged. This process continues until a desired level of accuracy is achieved.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The weight functions of the subdomains for the second level of the domain decomposition. In this example $T$ (the time to which the differential equation is solved) is one.\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:weights}{{4}{5}{The weight functions of the subdomains for the second level of the domain decomposition. In this example $T$ (the time to which the differential equation is solved) is one.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Contributions Made to the Project}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Summary}{5}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A simple diagram of the MFFBPINN algorithm for a 1D problem where the number of domains is doubled at each step. The collocation points (red) and the predictions from the previous levels are fed into the next level. Each circle is a PINN and its corresponding domain. The PINN on the zeroth level only takes in the collocation points and is therefore a single fidelity PINN. The PINNs on the subsequent levels take in the collocation points and the prediction of the previous layer, making them multifidelity PINNs.\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:mffbpinn}{{5}{6}{A simple diagram of the MFFBPINN algorithm for a 1D problem where the number of domains is doubled at each step. The collocation points (red) and the predictions from the previous levels are fed into the next level. Each circle is a PINN and its corresponding domain. The PINN on the zeroth level only takes in the collocation points and is therefore a single fidelity PINN. The PINNs on the subsequent levels take in the collocation points and the prediction of the previous layer, making them multifidelity PINNs.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Results}{6}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Pendulum Equation}{7}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{subfig:a}{{6a}{7}{\relax }{figure.caption.7}{}}
\newlabel{sub@subfig:a}{{a}{7}{\relax }{figure.caption.7}{}}
\newlabel{subfig:b}{{6b}{7}{\relax }{figure.caption.7}{}}
\newlabel{sub@subfig:b}{{b}{7}{\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Here we test two MFFBPINNs on a damped pendulum. Both methods converge to the true solution of the pendulum for up to $T=22$.\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:pend_res}{{6}{7}{Here we test two MFFBPINNs on a damped pendulum. Both methods converge to the true solution of the pendulum for up to $T=22$.\relax }{figure.caption.7}{}}
\newlabel{subfig:a}{{7a}{7}{\relax }{figure.caption.8}{}}
\newlabel{sub@subfig:a}{{a}{7}{\relax }{figure.caption.8}{}}
\newlabel{subfig:b}{{7b}{7}{\relax }{figure.caption.8}{}}
\newlabel{sub@subfig:b}{{b}{7}{\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces (a) The errors for the pendulum results in Figure \ref {fig:pend_res} by level. Level 0 corresponds to the single fidelity network. The performance is comparable. The addition of domain decomposition does not affect the convergence much. (b) This is the table of hyperparameters for both MFFBPINNs. All hyperparameters were the same for these tests.\relax }}{7}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Wave Equation}{8}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{subfig:a}{{8a}{8}{\relax }{figure.caption.9}{}}
\newlabel{sub@subfig:a}{{a}{8}{\relax }{figure.caption.9}{}}
\newlabel{subfig:b}{{8b}{8}{\relax }{figure.caption.9}{}}
\newlabel{sub@subfig:b}{{b}{8}{\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Here we test two MFFBPINNs on the wave equation. The MFFBPINN without the domain decomposition converges here. The other does not. It seems that adding the domain decomposition can complicate convergence.\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:wave_res}{{8}{8}{Here we test two MFFBPINNs on the wave equation. The MFFBPINN without the domain decomposition converges here. The other does not. It seems that adding the domain decomposition can complicate convergence.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The errors for the wave equation results in Figure \ref {fig:wave_res} by level. The addition of domain decomposition seems to stymy the convergence. The (a) was trained for three levels and (b) was trained for 6.\relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{subfig:a}{{10a}{9}{\relax }{figure.caption.11}{}}
\newlabel{sub@subfig:a}{{a}{9}{\relax }{figure.caption.11}{}}
\newlabel{subfig:b}{{10b}{9}{\relax }{figure.caption.11}{}}
\newlabel{sub@subfig:b}{{b}{9}{\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Here we have the hyperparameters for (a) the MFFBPINN without domain decomposition and (b) the MFFBPINN with domain decomposition. The derivative weight penalizes the network for not having the correct derivative at $t=0$.\relax }}{9}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Allen-Cahn Equation}{10}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{subfig:a}{{11a}{10}{\relax }{figure.caption.12}{}}
\newlabel{sub@subfig:a}{{a}{10}{\relax }{figure.caption.12}{}}
\newlabel{subfig:b}{{11b}{10}{\relax }{figure.caption.12}{}}
\newlabel{sub@subfig:b}{{b}{10}{\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Here we test two MFFBPINNs on the Allen-Cahn equation. (a) outperforms (b), however neither converge to the true solution.\relax }}{10}{figure.caption.12}\protected@file@percent }
\newlabel{fig:cahn_res}{{11}{10}{Here we test two MFFBPINNs on the Allen-Cahn equation. (a) outperforms (b), however neither converge to the true solution.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The errors for the Allen-Cahn equation results in Figure \ref {fig:cahn_res} by level. This is the only problem we tested with heterogeneous structure. It stands to reason that the domain decomposition would help the convergence.\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{subfig:a}{{13a}{11}{\relax }{figure.caption.14}{}}
\newlabel{sub@subfig:a}{{a}{11}{\relax }{figure.caption.14}{}}
\newlabel{subfig:b}{{13b}{11}{\relax }{figure.caption.14}{}}
\newlabel{sub@subfig:b}{{b}{11}{\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Here we have the hyperparameters for (a) the MFFBPINN without domain decomposition and (b) the MFFBPINN with domain decomposition. The derivative weight penalizes the network for not having the correct derivative at $t=0$.\relax }}{11}{figure.caption.14}\protected@file@percent }
\bibstyle{plain}
\bibdata{refs}
\@writefile{toc}{\contentsline {section}{\numberline {4}New Skills and Knowledge}{12}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experience and Impact on My Career}{12}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Relevance to the Mission of NSF}{12}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgements}{13}{section.7}\protected@file@percent }
\bibcite{fbpinns}{1}
\bibcite{mfdeeponets}{2}
\bibcite{mfpinns}{3}
\bibcite{pinns}{4}
\bibcite{fixedpts}{5}
\gdef \@abspage@last{15}
